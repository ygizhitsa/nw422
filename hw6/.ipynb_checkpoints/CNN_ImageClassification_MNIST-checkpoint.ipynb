{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with Convolutional Neural Networks\n",
    "\n",
    "Original code at: https://github.com/ageron/handson-ml/blob/master/13_convolutional_neural_networks.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the notebook compatible with both Python 2 and 3\n",
    "\n",
    "http://python-future.org/compatible_idioms.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot graphs inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "1.16.2\n",
      "3.0.3\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "print(matplotlib.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and extract the MNIST libraries\n",
    "\n",
    "The original site where this dataset is available: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist_data/train-images-idx3-ubyte.gz\n",
      "Extracting mnist_data/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Store the MNIST data in mnist_data/\n",
    "mnist = input_data.read_data_sets(\"mnist_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function to display one digit image\n",
    "\n",
    "Reshape the data from 1-D array to a 2-D array of 28x28 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_digit(digit):\n",
    "    plt.imshow(digit.reshape(28, 28), cmap=\"Greys\", interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access the training and test data and the corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_digits, training_labels = mnist.train.next_batch(10000)\n",
    "test_digits, test_labels = mnist.test.next_batch(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x102b5e390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_digit(training_digits[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 1, 3, 5, 2], dtype=uint8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensions of MNIST images\n",
    "\n",
    "* Each image is 28x28 pixels\n",
    "* The images are grayscale and have just one channel\n",
    "* The number of inputs is equal to the number of pixels in each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "height = 28\n",
    "width = 28\n",
    "channels = 1\n",
    "n_inputs = height * width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up a CNN with 2 convolutional layers and one max pool layer\n",
    "\n",
    "* Specify the number of **feature maps** in each layer, a feature map highlights that area in an image which is most similar to the filter applied\n",
    "* The kernel size indicates the **dimensions of the filter** which is applied to the image. The filter variables are created for you and initialized randomly\n",
    "* The stride is the steps by which the filter moves over the input, the **distance between two receptive fields on the input**\n",
    "* \"SAME\" padding indicates that the convolutional layer **uses zero padding** on the inputs and will consider all inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv1_feature_maps = 32\n",
    "conv1_kernel_size = 3\n",
    "conv1_stride = 1\n",
    "conv1_pad = \"SAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv2_feature_maps = 64\n",
    "conv2_kernel_size = 3\n",
    "conv2_stride = 2\n",
    "conv2_pad = \"SAME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool3_feature_maps = conv2_feature_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One fully connected layer\n",
    "\n",
    "* 64 neurons in the layer\n",
    "* 10 outputs corresponding to the digits 0-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_fullyconn1 = 64\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders for training data and labels\n",
    "\n",
    "* The training dataset placeholder can have any number of instances and each instance is an array of n_inputs = 28x28 = 784 pixels\n",
    "* The images are fed to the convolutional layer as a 4D tensor *[batch_size, height, width, channels]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, n_inputs], name=\"X\")\n",
    "X_reshaped = tf.reshape(X, shape=[-1, height, width, channels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.int32, shape=[None], name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect the convolutional layers\n",
    "\n",
    "* Each layer uses the ReLU activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = tf.layers.conv2d(X_reshaped, filters=conv1_feature_maps,\n",
    "                         kernel_size=conv1_kernel_size,\n",
    "                         strides=conv1_stride, padding=conv1_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conv2 = tf.layers.conv2d(conv1, filters=conv2_feature_maps, \n",
    "                         kernel_size=conv2_kernel_size,\n",
    "                         strides=conv2_stride, padding=conv2_pad,\n",
    "                         activation=tf.nn.relu, name=\"conv2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect a max pooling layer\n",
    "\n",
    "* The filter is a 2x2 filter\n",
    "* The stride is 2 both horizontally and vertically\n",
    "* This results in an image that is **1/4th the size of the original image**\n",
    "* Reshape the pooled layer to be a **1-D vector (flatten it)**. It now has only 1/4th the number of pixels in each feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool3 = tf.nn.max_pool(conv2,\n",
    "                       ksize=[1, 2, 2, 1],\n",
    "                       strides=[1, 2, 2, 1],\n",
    "                       padding=\"VALID\")\n",
    "\n",
    "pool3_flat = tf.reshape(pool3, shape=[-1, pool3_feature_maps * 7 * 7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullyconn1 = tf.layers.dense(pool3_flat, n_fullyconn1,\n",
    "                             activation=tf.nn.relu, name=\"fc1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final output layer with softmax activation\n",
    "\n",
    "Do not apply the softmax activation to this layer. The *tf.nn.sparse_softmax_cross_entropy_with_logits* will apply the softmax activation as well as calculate the cross-entropy as our cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.layers.dense(fullyconn1, n_outputs, name=\"output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy as a cost function\n",
    "\n",
    "* Use the Adam optimizer which in most cases performs better than the simple gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,\n",
    "                                                          labels=y)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check correctness and accuracy of the prediction\n",
    "\n",
    "* Check whether the highest probability output in logits is equal to the y-label\n",
    "* Check the accuracy across all predictions (How many predictions did we get right?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with the training data, measure accuracy with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.98 Test accuracy: 0.9781\n",
      "1 Train accuracy: 0.98 Test accuracy: 0.9849\n",
      "2 Train accuracy: 0.98 Test accuracy: 0.9835\n",
      "3 Train accuracy: 1.0 Test accuracy: 0.9866\n",
      "4 Train accuracy: 0.99 Test accuracy: 0.9883\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_test = accuracy.eval(feed_dict={X: mnist.test.images, y: mnist.test.labels})\n",
    "        print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "\n",
    "        save_path = saver.save(sess, \"./my_mnist_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
